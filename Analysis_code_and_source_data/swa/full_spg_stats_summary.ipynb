{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3b64ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------- Standard Imports --------------------------#\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import acr\n",
    "import warnings\n",
    "import pingouin as pg\n",
    "from scipy.stats import shapiro, normaltest\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "probe_ord = ['NNXr', 'NNXo']\n",
    "\n",
    "from acr.plots import pub, lrg\n",
    "\n",
    "plt.style.use('fast')\n",
    "pub()\n",
    "plt.rcParams['xtick.bottom'] = False\n",
    "#--------------------------------- Import Publication Functions ---------------------------------#\n",
    "pub_utils = acr.utils.import_publication_functions('/home/kdriessen/gh_master/PUBLICATION__ACR/pub_utils.py', 'pub_utils')\n",
    "from pub_utils import *\n",
    "data_agg = acr.utils.import_publication_functions('/home/kdriessen/gh_master/PUBLICATION__ACR/data_agg.py', 'data_agg')\n",
    "from data_agg import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b38a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.multitest import multipletests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7ba2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './statistical_source_data/ACR_full_spg_source_data.csv'\n",
    "dat = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0491b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# statistical-tests for all frequencies\n",
    "stat_dfs = []\n",
    "t_tests = {}\n",
    "freqs = []\n",
    "p_vals = []\n",
    "for freq in dat['freq_bin'].unique():\n",
    "    freqs.append(freq)\n",
    "    freq_df = dat.loc[dat['freq_bin'] == freq].sort_values(['subject'])\n",
    "    nnxr = freq_df['contra_control_power'].values\n",
    "    nnxo = freq_df['optrode_power'].values\n",
    "    \n",
    "    # will use wilcoxon across all frequencies to avoid differing normality assumptions across different frequencies, etc\n",
    "    stat = pg.wilcoxon(nnxr, nnxo)\n",
    "    p_vals.append(stat['p-val'][0])\n",
    "    \n",
    "    # build stat df\n",
    "    stat = stat.drop(columns=['CLES', 'alternative'])\n",
    "    stat['freq_bin'] = freq\n",
    "    stat['Test Type'] = 'wilcoxon signed-rank'\n",
    "    stat.rename(columns={'W-val': 'Test Statistic', 'p-val': 'p-value-uncorrected', 'RBC': 'Effect Size'}, inplace=True)\n",
    "    stat['Effect Size Method'] = 'RBC'\n",
    "    stat['N'] = len(nnxr)\n",
    "    order = ['N', 'Test Type', 'Test Statistic', 'p-value-uncorrected', 'Effect Size Method', 'Effect Size', 'freq_bin']\n",
    "    stat = stat[order]\n",
    "    stat['p-value-corrected'] = 'NA'\n",
    "    stat_dfs.append(stat)\n",
    "stats = pd.concat(stat_dfs)\n",
    "# Correct for multiple Comparisons using Benjamini–Hochberg False discovery rate correction \n",
    "rej, p_fdr, _, _ = multipletests(p_vals, alpha=0.05, method='fdr_bh')\n",
    "\n",
    "for p_cor, freq in zip(p_fdr, freqs):\n",
    "    stats.loc[stats['freq_bin'] == freq, 'p-value-corrected'] = p_cor\n",
    "    if p_cor<0.05:\n",
    "        print(round(freq, 1), round(p_cor, 3))\n",
    "stats['correction_method'] = 'Benjamini-Hochberg FDR'\n",
    "new_path = path.replace('.csv', '--Full_stats.csv')\n",
    "stats.to_csv(new_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4491a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './statistical_source_data/SOM_full_spg_source_data.csv'\n",
    "dat = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5721554",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# statistical-tests for all frequencies\n",
    "stat_dfs = []\n",
    "t_tests = {}\n",
    "freqs = []\n",
    "p_vals = []\n",
    "for freq in dat['freq_bin'].unique():\n",
    "    freqs.append(freq)\n",
    "    freq_df = dat.loc[dat['freq_bin'] == freq].sort_values(['subject'])\n",
    "    nnxr = freq_df['contra_control_power'].values\n",
    "    nnxo = freq_df['optrode_power'].values\n",
    "    \n",
    "    # will use wilcoxon across all frequencies to avoid differing normality assumptions across different frequencies, etc\n",
    "    stat = pg.wilcoxon(nnxr, nnxo)\n",
    "    p_vals.append(stat['p-val'][0])\n",
    "    \n",
    "    # build stat df\n",
    "    stat = stat.drop(columns=['CLES', 'alternative'])\n",
    "    stat['freq_bin'] = freq\n",
    "    stat['Test Type'] = 'wilcoxon signed-rank'\n",
    "    stat.rename(columns={'W-val': 'Test Statistic', 'p-val': 'p-value-uncorrected', 'RBC': 'Effect Size'}, inplace=True)\n",
    "    stat['Effect Size Method'] = 'RBC'\n",
    "    stat['N'] = len(nnxr)\n",
    "    order = ['N', 'Test Type', 'Test Statistic', 'p-value-uncorrected', 'Effect Size Method', 'Effect Size', 'freq_bin']\n",
    "    stat = stat[order]\n",
    "    stat['p-value-corrected'] = 'NA'\n",
    "    stat_dfs.append(stat)\n",
    "stats = pd.concat(stat_dfs)\n",
    "# Correct for multiple Comparisons using Benjamini–Hochberg False discovery rate correction \n",
    "rej, p_fdr, _, _ = multipletests(p_vals, alpha=0.05, method='fdr_bh')\n",
    "\n",
    "for p_cor, freq in zip(p_fdr, freqs):\n",
    "    stats.loc[stats['freq_bin'] == freq, 'p-value-corrected'] = p_cor\n",
    "    if p_cor<0.05:\n",
    "        print(round(freq, 1), round(p_cor, 3))\n",
    "stats['correction_method'] = 'Benjamini-Hochberg FDR'\n",
    "new_path = path.replace('.csv', '--Full_stats.csv')\n",
    "stats.to_csv(new_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8315c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './statistical_source_data/CTRL_full_spg_source_data.csv'\n",
    "dat = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02cd689",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# statistical-tests for all frequencies\n",
    "stat_dfs = []\n",
    "t_tests = {}\n",
    "freqs = []\n",
    "p_vals = []\n",
    "for freq in dat['freq_bin'].unique():\n",
    "    freqs.append(freq)\n",
    "    freq_df = dat.loc[dat['freq_bin'] == freq].sort_values(['subject'])\n",
    "    nnxr = freq_df['contra_control_power'].values\n",
    "    nnxo = freq_df['optrode_power'].values\n",
    "    \n",
    "    # will use wilcoxon across all frequencies to avoid differing normality assumptions across different frequencies, etc\n",
    "    stat = pg.wilcoxon(nnxr, nnxo)\n",
    "    p_vals.append(stat['p-val'][0])\n",
    "    \n",
    "    # build stat df\n",
    "    stat = stat.drop(columns=['CLES', 'alternative'])\n",
    "    stat['freq_bin'] = freq\n",
    "    stat['Test Type'] = 'wilcoxon signed-rank'\n",
    "    stat.rename(columns={'W-val': 'Test Statistic', 'p-val': 'p-value-uncorrected', 'RBC': 'Effect Size'}, inplace=True)\n",
    "    stat['Effect Size Method'] = 'RBC'\n",
    "    stat['N'] = len(nnxr)\n",
    "    order = ['N', 'Test Type', 'Test Statistic', 'p-value-uncorrected', 'Effect Size Method', 'Effect Size', 'freq_bin']\n",
    "    stat = stat[order]\n",
    "    stat['p-value-corrected'] = 'NA'\n",
    "    stat_dfs.append(stat)\n",
    "stats = pd.concat(stat_dfs)\n",
    "# Correct for multiple Comparisons using Benjamini–Hochberg False discovery rate correction \n",
    "rej, p_fdr, _, _ = multipletests(p_vals, alpha=0.05, method='fdr_bh')\n",
    "\n",
    "for p_cor, freq in zip(p_fdr, freqs):\n",
    "    stats.loc[stats['freq_bin'] == freq, 'p-value-corrected'] = p_cor\n",
    "    if p_cor<0.05:\n",
    "        print(round(freq, 1), round(p_cor, 3))\n",
    "stats['correction_method'] = 'Benjamini-Hochberg FDR'\n",
    "new_path = path.replace('.csv', '--Full_stats.csv')\n",
    "stats.to_csv(new_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3353a4ec",
   "metadata": {},
   "source": [
    "HALO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ee2b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './tonic/statistical_source_data/HALO_full_spg_source_data--TONIC.csv'\n",
    "dat = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3a8300",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# statistical-tests for all frequencies\n",
    "stat_dfs = []\n",
    "t_tests = {}\n",
    "freqs = []\n",
    "p_vals = []\n",
    "for freq in dat['freq_bin'].unique():\n",
    "    freqs.append(freq)\n",
    "    freq_df = dat.loc[dat['freq_bin'] == freq].sort_values(['subject'])\n",
    "    nnxr = freq_df['contra_control_power'].values\n",
    "    nnxo = freq_df['optrode_power'].values\n",
    "    \n",
    "    # will use wilcoxon across all frequencies to avoid differing normality assumptions across different frequencies, etc\n",
    "    stat = pg.wilcoxon(nnxr, nnxo)\n",
    "    p_vals.append(stat['p-val'][0])\n",
    "    \n",
    "    # build stat df\n",
    "    stat = stat.drop(columns=['CLES', 'alternative'])\n",
    "    stat['freq_bin'] = freq\n",
    "    stat['Test Type'] = 'wilcoxon signed-rank'\n",
    "    stat.rename(columns={'W-val': 'Test Statistic', 'p-val': 'p-value-uncorrected', 'RBC': 'Effect Size'}, inplace=True)\n",
    "    stat['Effect Size Method'] = 'RBC'\n",
    "    stat['N'] = len(nnxr)\n",
    "    order = ['N', 'Test Type', 'Test Statistic', 'p-value-uncorrected', 'Effect Size Method', 'Effect Size', 'freq_bin']\n",
    "    stat = stat[order]\n",
    "    stat['p-value-corrected'] = 'NA'\n",
    "    stat_dfs.append(stat)\n",
    "stats = pd.concat(stat_dfs)\n",
    "# Correct for multiple Comparisons using Benjamini–Hochberg False discovery rate correction \n",
    "rej, p_fdr, _, _ = multipletests(p_vals, alpha=0.05, method='fdr_bh')\n",
    "\n",
    "for p_cor, freq in zip(p_fdr, freqs):\n",
    "    stats.loc[stats['freq_bin'] == freq, 'p-value-corrected'] = p_cor\n",
    "    if p_cor<0.05:\n",
    "        print(round(freq, 1), round(p_cor, 3))\n",
    "stats['correction_method'] = 'Benjamini-Hochberg FDR'\n",
    "new_path = path.replace('.csv', '--Full_stats.csv')\n",
    "stats.to_csv(new_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ced34c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "off_ind",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
